{"cells":[{"cell_type":"code","source":["# Take a look at the file system\ndisplay(dbutils.fs.ls(\"/databricks-datasets/samples/docs/\"))."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/samples/docs/README.md</td><td>README.md</td><td>3137</td></tr></tbody></table></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# transformation\ntextFile = spark.read.text(\"/databricks-datasets/samples/docs/README.md\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# action\ntextFile.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: 65</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# display columns\ntextFile.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [&#39;value&#39;]</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Output the first line from the text file\ntextFile.first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: Row(value=&#39;Welcome to the Spark documentation!&#39;)</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Filter all of the lines within the DataFrame\nlinesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))\n\n# Perform a count (action) \nc = linesWithSpark.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# display\nlinesWithSpark.take(c)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [Row(value=&#39;Welcome to the Spark documentation!&#39;),\n Row(value=&#39;This readme will walk you through navigating and building the Spark documentation, which is included&#39;),\n Row(value=&#39;here with the Spark source code. You can also find documentation specific to release versions of&#39;),\n Row(value=&#39;Spark at http://spark.apache.org/documentation.html.&#39;),\n Row(value=&#39;whichever version of Spark you currently have checked out of revision control.&#39;),\n Row(value=&#39;The Spark documentation build uses a number of tools to build HTML docs and API docs in Scala,&#39;),\n Row(value=&#39;We include the Spark documentation as part of the source (as opposed to using a hosted wiki, such as&#39;),\n Row(value=&#39;You can build just the Spark scaladoc by running `build/sbt unidoc` from the SPARK_PROJECT_ROOT directory.&#39;),\n Row(value=&#39;Similarly, you can build just the PySpark docs by running `make html` from the&#39;),\n Row(value=&#39;public in `__init__.py`. The SparkR docs can be built by running SPARK_PROJECT_ROOT/R/create-docs.sh.&#39;),\n Row(value=&#39;Spark subprojects into the `docs` directory (and then also into the `_site` directory). We use a&#39;),\n Row(value=&#39;PySpark docs [Sphinx](http://sphinx-doc.org/).&#39;)]</div>"]}}],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"portonoto","notebookId":83445298684628},"nbformat":4,"nbformat_minor":0}
