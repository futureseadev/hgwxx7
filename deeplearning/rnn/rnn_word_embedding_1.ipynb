{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from itertools import chain\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "TRAIN_FRACTION = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import HTML\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import get_data, get_embeddings, find_closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Training Data\n",
    "\n",
    "* Using patent abstracts from patent search for neural network\n",
    "* 3000+ patents total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>5535303</td>\n",
       "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>1993-10-19</td>\n",
       "      <td>5255349</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An optical information processor for use as a ...</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>5383042</td>\n",
       "      <td>3 layer liquid crystal neural network with out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>6169981</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2003-06-17</td>\n",
       "      <td>6581048</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract patent_date  \\\n",
       "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
       "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
       "2  An optical information processor for use as a ...  1995-01-17   \n",
       "3  A method and system for intelligent control of...  2001-01-02   \n",
       "4  A method and system for intelligent control of...  2003-06-17   \n",
       "\n",
       "  patent_number                                       patent_title  \n",
       "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
       "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
       "2       5383042  3 layer liquid crystal neural network with out...  \n",
       "3       6169981  3-brain architecture for an intelligent decisi...  \n",
       "4       6581048  3-brain architecture for an intelligent decisi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16192 unique words.\n",
      "There are 318563 sequences.\n"
     ]
    }
   ],
   "source": [
    "training_dict, word_idx, idx_word, sequences = get_data('../data/neural_network_patent_query.csv', training_len = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequences of text are represented as integers\n",
    "    * `word_idx` maps words to integers\n",
    "    * `idx_word` maps integers to words\n",
    "* Features are integer sequences of length 50\n",
    "* Label is next word in sequence\n",
    "* Labels are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
       "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
       "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
       "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
       "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
       "           81,   183,     4,    13,   117],\n",
       "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
       "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
       "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
       "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
       "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
       "         9292,   244,    14,    22, 13944]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dict['X_train'][:2]\n",
    "training_dict['y_train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
      "\n",
      "Label: is\n",
      "\n",
      "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
      "\n",
      "Label: group\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text) + '\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Recurrent Neural Network\n",
    "\n",
    "* Embedding dimension = 100\n",
    "* 64 LSTM cells in one layer\n",
    "    * Dropout and recurrent dropout for regularization\n",
    "* Fully connected layer with 64 units on top of LSTM\n",
    "     * 'relu' activation\n",
    "* Drop out for regularization\n",
    "* Output layer produces prediction for each word\n",
    "    * 'softmax' activation\n",
    "* Adam optimizer with defaults\n",
    "* Categorical cross entropy loss\n",
    "* Monitor accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1619200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16192)             1052480   \n",
      "=================================================================\n",
      "Total params: 2,718,080\n",
      "Trainable params: 2,718,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(word_idx) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 222994 samples, validate on 95569 samples\n",
      "Epoch 1/5\n",
      "222994/222994 [==============================] - 371s 2ms/step - loss: 7.2916 - acc: 0.0593 - val_loss: 6.3476 - val_acc: 0.0771\n",
      "Epoch 2/5\n",
      "222994/222994 [==============================] - 381s 2ms/step - loss: 6.3136 - acc: 0.0751 - val_loss: 6.2629 - val_acc: 0.0771\n",
      "Epoch 3/5\n",
      "222994/222994 [==============================] - 370s 2ms/step - loss: 6.2444 - acc: 0.0751 - val_loss: 6.2553 - val_acc: 0.0771\n",
      "Epoch 4/5\n",
      "222994/222994 [==============================] - 370s 2ms/step - loss: 6.2080 - acc: 0.0762 - val_loss: 6.2017 - val_acc: 0.0771\n",
      "Epoch 5/5\n",
      "222994/222994 [==============================] - 364s 2ms/step - loss: 6.1064 - acc: 0.0891 - val_loss: 6.0645 - val_acc: 0.1017\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs=5, batch_size=2048, \n",
    "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: Log Loss and Accuracy on training data\n",
      "222994/222994 [==============================] - 106s 477us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.924870105172796, 0.09993093985607225]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance: Log Loss and Accuracy on validation data\n",
      "95569/95569 [==============================] - 46s 484us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.064511908455439, 0.10173801127474089]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Model Performance: Log Loss and Accuracy on training data')\n",
    "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
    "\n",
    "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
    "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Embeddings\n",
    "\n",
    "As a final piece of model inspection, we can look at the embeddings and find the words closest to a query word in the embedding space. This gives us an idea of what the network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16192, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embeddings(model)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the vocabulary is now represented as a 100-dimensional vector. This could be reduced to 2 or 3 dimensions for visualization. It can also be used to find the closest word to a query word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: network\n",
      "\n",
      "Word: network         Cosine Similarity: 1.0\n",
      "Word: neural          Cosine Similarity: 0.785099983215332\n",
      "Word: based           Cosine Similarity: 0.758899986743927\n",
      "Word: so              Cosine Similarity: 0.7515000104904175\n",
      "Word: corresponding   Cosine Similarity: 0.7384999990463257\n",
      "Word: associated      Cosine Similarity: 0.7196999788284302\n",
      "Word: signal          Cosine Similarity: 0.715499997138977\n",
      "Word: order           Cosine Similarity: 0.704200029373169\n",
      "Word: value           Cosine Similarity: 0.7010999917984009\n",
      "Word: networks        Cosine Similarity: 0.7002999782562256\n"
     ]
    }
   ],
   "source": [
    "find_closest('network', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: web\n",
      "\n",
      "Word: web             Cosine Similarity: 1.0\n",
      "Word: .sup            Cosine Similarity: 0.8828999996185303\n",
      "Word: drug            Cosine Similarity: 0.8813999891281128\n",
      "Word: living          Cosine Similarity: 0.8738999962806702\n",
      "Word: &#8220          Cosine Similarity: 0.8738999962806702\n",
      "Word: neurotrophic    Cosine Similarity: 0.8737000226974487\n",
      "Word: Fuzzy           Cosine Similarity: 0.8734999895095825\n",
      "Word: remains         Cosine Similarity: 0.8729000091552734\n",
      "Word: dopamine        Cosine Similarity: 0.8718000054359436\n",
      "Word: simultaneous    Cosine Similarity: 0.8708999752998352\n"
     ]
    }
   ],
   "source": [
    "find_closest('web', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
