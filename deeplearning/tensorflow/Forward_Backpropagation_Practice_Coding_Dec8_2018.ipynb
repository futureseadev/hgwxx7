{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Dataset comes from here:\n",
    "#https://github.com/Mashimo/datascience/raw/master/datasets/train_catvnoncat.h5\n",
    "# This code along with explanation is here:\n",
    "# https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76\n",
    "import h5py \n",
    "# Store huge amounts of numerical data, and easily manipulate that data from NumPy. For example, you can slice into multi-terabyte datasets stored on disk, as if they wer\n",
    "# https://www.h5py.org/\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims):\n",
    "    np.random.seed(1)               \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            \n",
    "\n",
    "    for l in range(1, L):           \n",
    "        parameters[\"W\" + str(l)] = np.random.randn(\n",
    "            layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "        assert parameters[\"W\" + str(l)].shape == (\n",
    "            layers_dims[l], layers_dims[l - 1])\n",
    "        assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    return A, Z\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A, Z\n",
    "\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    A = np.maximum(0.1 * Z, Z)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the 4 activation functions\n",
    "z = np.linspace(-10, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes post-activation outputs\n",
    "A_sigmoid, z = sigmoid(z)\n",
    "A_tanh, z = tanh(z)\n",
    "A_relu, z = relu(z)\n",
    "A_leaky_relu, z = leaky_relu(z)\n",
    "\n",
    "# Plot sigmoid\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(z, A_sigmoid, label=\"Function\")\n",
    "plt.plot(z, A_sigmoid * (1 - A_sigmoid), label = \"Derivative\") \n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(r\"$\\frac{1}{1 + e^{-z}}$\")\n",
    "plt.title(\"Sigmoid Function\", fontsize=16)\n",
    "\n",
    "\n",
    "# Plot tanh\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(z, A_tanh, 'b', label = \"Function\")\n",
    "plt.plot(z, 1 - np.square(A_tanh), 'r',label=\"Derivative\") \n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(r\"$\\frac{e^z - e^{-z}}{e^z + e^{-z}}$\") \n",
    "plt.title(\"Hyperbolic Tangent Function\", fontsize=16)\n",
    "\n",
    "\n",
    "# plot relu\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(z, A_relu, 'g')\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(r\"$max\\{0, z\\}$\")\n",
    "plt.title(\"ReLU Function\", fontsize=16)\n",
    "\n",
    "# plot leaky relu\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(z, A_leaky_relu, 'y')\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(r\"$max\\{0.1z, z\\}$\")\n",
    "plt.title(\"Leaky ReLU Function\", fontsize=16)\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.53978687e-05, 5.55606489e-05, 6.79983174e-05, 8.32200197e-05,\n",
       "        1.01848815e-04, 1.24647146e-04, 1.52547986e-04, 1.86692945e-04,\n",
       "        2.28478855e-04, 2.79614739e-04, 3.42191434e-04, 4.18766684e-04,\n",
       "        5.12469082e-04, 6.27124987e-04, 7.67413430e-04, 9.39055039e-04,\n",
       "        1.14904229e-03, 1.40591988e-03, 1.72012560e-03, 2.10440443e-03,\n",
       "        2.57431039e-03, 3.14881358e-03, 3.85103236e-03, 4.70911357e-03,\n",
       "        5.75728612e-03, 7.03711536e-03, 8.59898661e-03, 1.05038445e-02,\n",
       "        1.28252101e-02, 1.56514861e-02, 1.90885420e-02, 2.32625358e-02,\n",
       "        2.83228820e-02, 3.44451957e-02, 4.18339400e-02, 5.07243606e-02,\n",
       "        6.13831074e-02, 7.41067363e-02, 8.92170603e-02, 1.07052146e-01,\n",
       "        1.27951705e-01, 1.52235823e-01, 1.80176593e-01, 2.11963334e-01,\n",
       "        2.47663801e-01, 2.87185901e-01, 3.30246430e-01, 3.76354517e-01,\n",
       "        4.24816868e-01, 4.74768924e-01, 5.25231076e-01, 5.75183132e-01,\n",
       "        6.23645483e-01, 6.69753570e-01, 7.12814099e-01, 7.52336199e-01,\n",
       "        7.88036666e-01, 8.19823407e-01, 8.47764177e-01, 8.72048295e-01,\n",
       "        8.92947854e-01, 9.10782940e-01, 9.25893264e-01, 9.38616893e-01,\n",
       "        9.49275639e-01, 9.58166060e-01, 9.65554804e-01, 9.71677118e-01,\n",
       "        9.76737464e-01, 9.80911458e-01, 9.84348514e-01, 9.87174790e-01,\n",
       "        9.89496155e-01, 9.91401013e-01, 9.92962885e-01, 9.94242714e-01,\n",
       "        9.95290886e-01, 9.96148968e-01, 9.96851186e-01, 9.97425690e-01,\n",
       "        9.97895596e-01, 9.98279874e-01, 9.98594080e-01, 9.98850958e-01,\n",
       "        9.99060945e-01, 9.99232587e-01, 9.99372875e-01, 9.99487531e-01,\n",
       "        9.99581233e-01, 9.99657809e-01, 9.99720385e-01, 9.99771521e-01,\n",
       "        9.99813307e-01, 9.99847452e-01, 9.99875353e-01, 9.99898151e-01,\n",
       "        9.99916780e-01, 9.99932002e-01, 9.99944439e-01, 9.99954602e-01]),\n",
       " array([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n",
       "         -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n",
       "         -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n",
       "         -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n",
       "         -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n",
       "         -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n",
       "         -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n",
       "         -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n",
       "         -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n",
       "         -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n",
       "         -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n",
       "         -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n",
       "         -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n",
       "          0.50505051,   0.70707071,   0.90909091,   1.11111111,\n",
       "          1.31313131,   1.51515152,   1.71717172,   1.91919192,\n",
       "          2.12121212,   2.32323232,   2.52525253,   2.72727273,\n",
       "          2.92929293,   3.13131313,   3.33333333,   3.53535354,\n",
       "          3.73737374,   3.93939394,   4.14141414,   4.34343434,\n",
       "          4.54545455,   4.74747475,   4.94949495,   5.15151515,\n",
       "          5.35353535,   5.55555556,   5.75757576,   5.95959596,\n",
       "          6.16161616,   6.36363636,   6.56565657,   6.76767677,\n",
       "          6.96969697,   7.17171717,   7.37373737,   7.57575758,\n",
       "          7.77777778,   7.97979798,   8.18181818,   8.38383838,\n",
       "          8.58585859,   8.78787879,   8.98989899,   9.19191919,\n",
       "          9.39393939,   9.5959596 ,   9.7979798 ,  10.        ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sigmoid, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions that will be used in L-model forward prop\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation_fn):\n",
    "    assert activation_fn == \"sigmoid\" or activation_fn == \"tanh\" or \\\n",
    "        activation_fn == \"relu\"\n",
    "\n",
    "    if activation_fn == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation_fn == \"tanh\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation_fn == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters, hidden_layers_activation_fn=\"relu\"):\n",
    "    A = X                           \n",
    "    caches = []                     \n",
    "    L = len(parameters) // 2        \n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(\n",
    "            A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)],\n",
    "            activation_fn=hidden_layers_activation_fn)\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(\n",
    "        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],\n",
    "        activation_fn=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert AL.shape == (1, X.shape[1])\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute cross-entropy cost\n",
    "def compute_cost(AL, y):\n",
    "    m = y.shape[1]              \n",
    "    cost = - (1 / m) * np.sum(\n",
    "        np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(dA, Z):\n",
    "    A, Z = sigmoid(Z)\n",
    "    dZ = dA * A * (1 - A)\n",
    "\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_gradient(dA, Z):\n",
    "    A, Z = tanh(Z)\n",
    "    dZ = dA * (1 - np.square(A))\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_gradient(dA, Z):\n",
    "    A, Z = relu(Z)\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define helper functions that will be used in L-model back-prop\n",
    "def linear_backword(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation_fn):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation_fn == \"sigmoid\":\n",
    "        dZ = sigmoid_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    elif activation_fn == \"tanh\":\n",
    "        dZ = tanh_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    elif activation_fn == \"relu\":\n",
    "        dZ = relu_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\"):\n",
    "    y = y.reshape(AL.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\n",
    "        \"db\" + str(L)] = linear_activation_backward(\n",
    "            dAL, caches[L - 1], \"sigmoid\")\n",
    "\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l - 1]\n",
    "        grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\n",
    "            \"db\" + str(l)] = linear_activation_backward(\n",
    "                grads[\"dA\" + str(l)], current_cache,\n",
    "                hidden_layers_activation_fn)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\n",
    "            \"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\n",
    "            \"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (Unable to open file: name = '../data/train_catvnoncat.h5', errno = 2, error message = 'no such file or directory', flags = 15, o_flags = a02)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/Users/eli/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/h5f.c:2123)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = '../data/train_catvnoncat.h5', errno = 2, error message = 'no such file or directory', flags = 1, o_flags = 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/Users/eli/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDONLY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/h5f.c:2123)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = '../data/train_catvnoncat.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c8b1ebf10955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/train_catvnoncat.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eli/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eli/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDONLY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid mode; must be one of r, r+, w, w-, x, a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create (/Users/ilan/minonda/conda-bld/h5py_1490026758621/work/h5py/h5f.c:2290)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (Unable to open file: name = '../data/train_catvnoncat.h5', errno = 2, error message = 'no such file or directory', flags = 15, o_flags = a02)"
     ]
    }
   ],
   "source": [
    "# Import training dataset\n",
    "train_dataset = h5py.File(\"../data/train_catvnoncat.h5\")\n",
    "X_train = np.array(train_dataset[\"train_set_x\"])\n",
    "y_train = np.array(train_dataset[\"train_set_y\"])\n",
    "\n",
    "test_dataset = h5py.File(\"../data/test_catvnoncat.h5\")\n",
    "X_test = np.array(test_dataset[\"test_set_x\"])\n",
    "y_test = np.array(test_dataset[\"test_set_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-37-24e2c9985e14>, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-24e2c9985e14>\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    probs, caches = L_model_forward(X, parameters, activation_fn)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Define the multi-layer model using all the helper functions we wrote before\n",
    "def L_layer_model(\n",
    "        X, y, layers_dims, learning_rate=0.01, num_iterations=3000,\n",
    "        print_cost=True, hidden_layers_activation_fn=\"relu\"):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # intialize cost list\n",
    "    cost_list = []\n",
    "    # iterate over num_iterations\n",
    "    for i in range(num_iterations):\n",
    "        # iterate over L-layers to get the final output and the cache\n",
    "        AL, caches = L_model_forward(\n",
    "            X, parameters, hidden_layers_activation_fn)\n",
    "\n",
    "        # compute cost to plot it\n",
    "        cost = compute_cost(AL, y)\n",
    "\n",
    "        # iterate over L-layers backward to get gradients\n",
    "        grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn)\n",
    "\n",
    "        # update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # append each 100th cost to the cost list\n",
    "        if (i + 1) % 100 == 0 and print_cost:\n",
    "            print(f\"The cost after {i + 1} iterations is: {cost:.4f}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost)\n",
    "            # plot the cost curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cost_list)\n",
    "    plt.xlabel(\"Iterations (per hundreds)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss curve for the learning rate = {learning_rate}\")\n",
    "\n",
    "    return parameters\n",
    "\n",
    "    def accuracy(X, parameters, y, activation_fn=\"relu\"):\n",
    "    probs, caches = L_model_forward(X, parameters, activation_fn)\n",
    "    labels = (probs >= 0.5) * 1\n",
    "    accuracy = np.mean(labels == y) * 100\n",
    "    return f\"The accuracy rate is: {accuracy:.2f}%.\"view raw\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-32-b1c9c4870b86>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-b1c9c4870b86>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for i in range(num_iterations):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
