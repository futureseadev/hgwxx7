{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_GEC_0.1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBnc6zY+YA6BSKldKatrGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveentn/hgwxx7/blob/master/nlp/grammar/bert_gec_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tlm_0fI13OY",
        "colab_type": "text"
      },
      "source": [
        "# Grammar Error Correction using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzA9f77F19Ad",
        "colab_type": "text"
      },
      "source": [
        "High level workflow\n",
        "\n",
        "• Tokenize the sentence using Spacy\n",
        "\n",
        "• Check for spelling errors using Hunspell\n",
        "\n",
        "• For all preposition, determiners & helper verbs, create a set of probable sentences\n",
        "\n",
        "• Create a set of sentences with each word “masked”, deleted or an additional determiner, preposition or helper verb added\n",
        "\n",
        "• Used BERT Masked Language Model to determine possible suggestions for masks\n",
        "\n",
        "• Use the GED model to select appropriate solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX7hUCCH14ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1a86b2b3-a3ad-49c0-d37c-e6a3202f7a77"
      },
      "source": [
        "# install pytorch_pretrained_bert the previous version of Pytorch-Transformers\n",
        "!pip install -U pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.59)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.59)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.59->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTfte3l32HaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6d6aZPz2Hc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d21d0dcd-8ab2-4707-9dd4-6793d36619f6"
      },
      "source": [
        "\n",
        "# Check to confirm that GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdn0oa4E2Hf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a7dcaa5-0caa-4fe2-add3-26044f02601c"
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 673363.86B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7fef6bb84da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnHDoPfY2HjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def check_GE(sents):\n",
        "    \"\"\"Check of the input sentences have grammatical errors\n",
        "\n",
        "    :param list: list of sentences\n",
        "    :return: error, probabilities\n",
        "    :rtype: (boolean, (float, float))\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create sentence) and label lists\n",
        "    # We need to add special tokens at the beginning and end of each sentence\n",
        "    # for BERT to work properly\n",
        "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sents]\n",
        "    labels =[0]\n",
        "\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # Padding Sentences\n",
        "    # Set the maximum sequence length. The longest sequence in our training set\n",
        "    # is 47, but we'll leave room on the end anyway.\n",
        "    #  In the original paper, the authors used a length of 512.\n",
        "    MAX_LEN = 128\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    # Pad our input tokens\n",
        "    input_ids = pad_sequences(\n",
        "        [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
        "        maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
        "        )\n",
        "\n",
        "    # Index Numbers and Padding\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "    # pad sentences\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                            dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
        "\n",
        "    # Attention masks\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    prediction_inputs = torch.tensor(input_ids)\n",
        "    prediction_masks = torch.tensor(attention_masks)\n",
        "    prediction_labels = torch.tensor(labels)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        logits = modelGED(prediction_inputs, token_type_ids=None, \n",
        "                      attention_mask=prediction_masks)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    # label_ids = b_labels.to(\"cpu\").numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    # true_labels.append(label_ids)\n",
        "\n",
        "    #    print(predictions)\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    #   print(flat_predictions)\n",
        "    prob_vals = flat_predictions\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "    # flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "    #   print(flat_predictions)\n",
        "    return flat_predictions, prob_vals"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVnFDLw-2Hle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load previously trained BERT Grammar Error Detection model\n",
        "\n",
        "# from self google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp './drive/My Drive/Colab Notebooks/S89A/bert-based-uncased-GED.pth' ."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phuvoU_A2HoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# CREDIT: https://stackoverflow.com/a/39225039\n",
        "#\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    print(\"Trying to fetch {}\".format(destination))\n",
        "\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in progress_bar(response.iter_content(CHUNK_SIZE)):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69VYdE_62HrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def progress_bar(some_iter):\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        return tqdm(some_iter)\n",
        "    except ModuleNotFoundError:\n",
        "        return some_iter"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz7PpYu13LUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99c4ddf4-c914-4950-c474-e971ac20ffe3"
      },
      "source": [
        "# load previously trained BERT Grammar Error Detection model\n",
        "\n",
        "# download from public google drive link\n",
        "download_file_from_google_drive(\"1al7v87aRxebSUCXrN2Sdd0jGUS0zZ3vn\", \"./bert-based-uncased-GED.pth\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trying to fetch ./bert-based-uncased-GED.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "13367it [00:04, 3189.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRtaIZ9A3OOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c044d97-d63c-4880-fd99-b8e673921b61"
      },
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'bert-based-uncased-GED.pth', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd-B4nPa3Sse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "from pytorch_pretrained_bert import BertForSequenceClassification\n",
        "\n",
        "modelGED = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
        "                                                      num_labels=2)\n",
        "\n",
        "# restore model\n",
        "modelGED.load_state_dict(torch.load('bert-based-uncased-GED.pth'))\n",
        "modelGED.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2D5TJXl3YeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights) for Masked Language Model (MLM)\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hTgeavO3h88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a42719d5-3340-4a90-82da-6c6bc68231e2"
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizerLarge = BertTokenizer.from_pretrained('bert-large-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 675103.08B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUSyIEDm3iDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c274115e-2af8-47b8-a5b8-7259eb3f4325"
      },
      "source": [
        "# install the packages for Hunspell\n",
        "\n",
        "!sudo apt-get install libhunspell-1.6-0 libhunspell-dev\n",
        "!pip install cyhunspell"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dictionaries-common emacsen-common hunspell-en-us libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  ispell | aspell | hunspell wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core\n",
            "The following NEW packages will be installed:\n",
            "  dictionaries-common emacsen-common hunspell-en-us libhunspell-1.6-0\n",
            "  libhunspell-dev libtext-iconv-perl\n",
            "0 upgraded, 6 newly installed, 0 to remove and 11 not upgraded.\n",
            "Need to get 738 kB of archives.\n",
            "After this operation, 2,935 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-dev amd64 1.6.2-1 [198 kB]\n",
            "Fetched 738 kB in 0s (8,207 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 144676 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../1-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../2-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../3-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../4-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libhunspell-dev:amd64.\n",
            "Preparing to unpack .../5-libhunspell-dev_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-dev:amd64 (1.6.2-1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libhunspell-dev:amd64 (1.6.2-1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Collecting cyhunspell\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/2f/7a750fe5b849c99aafdf4f4526e1e7f1074ef62814c6e677c2c0060caa58/cyhunspell-2.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 8.4MB/s \n",
            "\u001b[?25hCollecting cacheman>=2.0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/46/7a/d091d6337693ddb1fe1c15b18fdc1c750e0355aa79aa33268cdeaeea4458/CacheMan-2.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cacheman>=2.0.6->cyhunspell) (1.15.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from cacheman>=2.0.6->cyhunspell) (0.16.0)\n",
            "Requirement already satisfied: psutil>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cacheman>=2.0.6->cyhunspell) (5.4.8)\n",
            "Installing collected packages: cacheman, cyhunspell\n",
            "Successfully installed cacheman-2.1.0 cyhunspell-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSmYVyo3qhj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5b4b00ed-7bb2-40ef-b6a5-87526e7b14f6"
      },
      "source": [
        "from hunspell import Hunspell\n",
        "import os\n",
        "\n",
        "# download the gn_GB dictionary for hunspell\n",
        "download_file_from_google_drive(\"1jC5BVF9iZ0gmRQNmDcZnhfFdEYv8RNok\", \"./en_GB-large.dic\")\n",
        "download_file_from_google_drive(\"1g8PO8kdw-YmyOY_HxjnJ5FfdJFX4bsPv\", \"./en_GB-large.aff\")\n",
        "\n",
        "gb = Hunspell(\"en_GB-large\", hunspell_data_dir=\".\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trying to fetch ./en_GB-large.dic\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "27it [00:00, 1716.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Trying to fetch ./en_GB-large.aff\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:00, 964.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXFKNtpZ3qkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of common determiners\n",
        "# det = [\"\", \"the\", \"a\", \"an\"]\n",
        "det = ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', \n",
        "       'her', 'its', 'our', 'their', 'all', 'both', 'half', 'either', 'neither', \n",
        "       'each', 'every', 'other', 'another', 'such', 'what', 'rather', 'quite']\n",
        "\n",
        "# List of common prepositions\n",
        "prep = [\"about\", \"at\", \"by\", \"for\", \"from\", \"in\", \"of\", \"on\", \"to\", \"with\", \n",
        "        \"into\", \"during\", \"including\", \"until\", \"against\", \"among\", \n",
        "        \"throughout\", \"despite\", \"towards\", \"upon\", \"concerning\"]\n",
        "\n",
        "# List of helping verbs\n",
        "helping_verbs = ['am', 'is', 'are', 'was', 'were', 'being', 'been', 'be', \n",
        "                 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', \n",
        "                 'shall', 'should', 'may', 'might', 'must', 'can', 'could']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIAwhUms3iGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test sentences\n",
        "\n",
        "org_text = []\n",
        "org_text.append(\"They drank the pub .\")\n",
        "org_text.append(\"I am looking forway to see you soon .\")\n",
        "org_text.append(\"The cat sat at mat .\")\n",
        "org_text.append(\"Giant otters is an apex predator .\")\n",
        "org_text.append('There is no a doubt, tracking system has brought many benefits in this information age .')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1VlAP3g3iJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_spelling_set(org_text):\n",
        "    \"\"\" Create a set of sentences which have possible corrected spellings\n",
        "  \"\"\"\n",
        "\n",
        "    sent = org_text\n",
        "    sent = sent.lower()\n",
        "    sent = sent.strip().split()\n",
        "\n",
        "    nlp = spacy.load('en')\n",
        "    proc_sent = nlp.tokenizer.tokens_from_list(sent)\n",
        "    nlp.tagger(proc_sent)\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    for tok in proc_sent:\n",
        "\n",
        "    # check for spelling for alphanumeric\n",
        "\n",
        "        if tok.text.isalpha() and not gb.spell(tok.text):\n",
        "            new_sent = sent[:]\n",
        "\n",
        "      # append new sentences with possible corrections\n",
        "\n",
        "            for sugg in gb.suggest(tok.text):\n",
        "                new_sent[tok.i] = sugg\n",
        "                sentences.append(' '.join(new_sent))\n",
        "\n",
        "    spelling_sentences = sentences\n",
        "\n",
        "    # retain new sentences which have a\n",
        "    # minimum chance of correctness using BERT GED\n",
        "\n",
        "    new_sentences = []\n",
        "\n",
        "    for sent in spelling_sentences:\n",
        "        (no_error, prob_val) = check_GE([sent])\n",
        "        exps = [np.exp(i) for i in prob_val[0]]\n",
        "        sum_of_exps = sum(exps)\n",
        "        softmax = [j / sum_of_exps for j in exps]\n",
        "        if softmax[1] > 0.6:\n",
        "            new_sentences.append(sent)\n",
        "\n",
        "    # if no corrections, append the original sentence\n",
        "\n",
        "    if len(spelling_sentences) == 0:\n",
        "        spelling_sentences.append(' '.join(sent))\n",
        "\n",
        "    # eliminate dupllicates\n",
        "\n",
        "    [spelling_sentences.append(sent) for sent in new_sentences]\n",
        "    spelling_sentences = list(dict.fromkeys(spelling_sentences))\n",
        "\n",
        "    return spelling_sentences\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKFOzbPR4TSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_grammar_set(spelling_sentences):\n",
        "    \"\"\" create a new set of sentences with deleted determiners, \n",
        "      prepositions & helping verbs\n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "    new_sentences = []\n",
        "\n",
        "    for text in spelling_sentences:\n",
        "        sent = text.strip().split()\n",
        "        for i in range(len(sent)):\n",
        "            new_sent = sent[:]\n",
        "\n",
        "            if new_sent[i] not in list(set(det + prep + helping_verbs)):\n",
        "                continue\n",
        "\n",
        "            del new_sent[i]\n",
        "            text = ' '.join(new_sent)\n",
        "\n",
        "            # retain new sentences which have a\n",
        "            # minimum chance of correctness using BERT GED\n",
        "\n",
        "            (no_error, prob_val) = check_GE([text])\n",
        "            exps = [np.exp(i) for i in prob_val[0]]\n",
        "            sum_of_exps = sum(exps)\n",
        "            softmax = [j / sum_of_exps for j in exps]\n",
        "            if softmax[1] > 0.6:\n",
        "                new_sentences.append(text)\n",
        "\n",
        "    # eliminate dupllicates\n",
        "\n",
        "    [spelling_sentences.append(sent) for sent in new_sentences]\n",
        "    spelling_sentences = list(dict.fromkeys(spelling_sentences))\n",
        "    return spelling_sentences\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sA61yHp4g9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_mask_set(spelling_sentences):\n",
        "    \"\"\"For each input sentence create 2 sentences\n",
        "     (1) [MASK] each word\n",
        "     (2) [MASK] for each space between words\n",
        "    \"\"\"\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    for sent in spelling_sentences:\n",
        "        sent = sent.strip().split()\n",
        "        for i in range(len(sent)):\n",
        "\n",
        "            # (1) [MASK] each word\n",
        "\n",
        "            new_sent = sent[:]\n",
        "            new_sent[i] = '[MASK]'\n",
        "            text = ' '.join(new_sent)\n",
        "            new_sent = '[CLS] ' + text + ' [SEP]'\n",
        "            sentences.append(new_sent)\n",
        "\n",
        "            # (2) [MASK] for each space between words\n",
        "\n",
        "            new_sent = sent[:]\n",
        "            new_sent.insert(i, '[MASK]')\n",
        "            text = ' '.join(new_sent)\n",
        "            new_sent = '[CLS] ' + text + ' [SEP]'\n",
        "            sentences.append(new_sent)\n",
        "\n",
        "    return sentences\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68WoS7e_4rAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "def check_grammar(org_sent, sentences, spelling_sentences):\n",
        "    \"\"\" check grammar for the input sentences\n",
        "  \"\"\"\n",
        "\n",
        "    n = len(sentences)\n",
        "\n",
        "  # what is the tokenized value of [MASK]. Usually 103\n",
        "\n",
        "    text = '[MASK]'\n",
        "    tokenized_text = tokenizerLarge.tokenize(text)\n",
        "    mask_token = tokenizerLarge.convert_tokens_to_ids(tokenized_text)[0]\n",
        "\n",
        "    LM_sentences = []\n",
        "    new_sentences = []\n",
        "    i = 0  # current sentence number\n",
        "    l = len(org_sent.strip().split()) * 2  # l is no of sentencees\n",
        "    mask = False  # flag indicating if we are processing space MASK\n",
        "\n",
        "    for sent in sentences:\n",
        "        i += 1\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print('.')\n",
        "\n",
        "        # tokenize the text\n",
        "\n",
        "        tokenized_text = tokenizerLarge.tokenize(sent)\n",
        "        indexed_tokens = \\\n",
        "            tokenizerLarge.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        # Create the segments tensors.\n",
        "\n",
        "        segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "        # Predict all tokens\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        # index of the masked token\n",
        "\n",
        "        mask_index = (tokens_tensor\n",
        "                      == mask_token).nonzero()[0][1].item()\n",
        "\n",
        "        # predicted token\n",
        "\n",
        "        predicted_index = torch.argmax(predictions[0,\n",
        "                mask_index]).item()\n",
        "        predicted_token = \\\n",
        "            tokenizerLarge.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "        #second best prediction. Can you used to create more options\n",
        "        #second_index = torch.topk(predictions[0, mask_index], 2).indices[1].item()\n",
        "        #second_prediction = tokenizer.convert_ids_to_tokens([second_index])[0]\n",
        "\n",
        "        text = sent.strip().split()\n",
        "        mask_index = text.index('[MASK]')\n",
        "\n",
        "        if not mask:\n",
        "        # case of MASKed words\n",
        "            mask = True\n",
        "            text[mask_index] = predicted_token\n",
        "            try:\n",
        "                # retrieve original word\n",
        "                org_word = spelling_sentences[i\n",
        "                        // l].strip().split()[mask_index - 1]\n",
        "            except:\n",
        "                print ('! exception')\n",
        "                continue\n",
        "\n",
        "            # use SequenceMatcher to see if predicted word is similar to original word\n",
        "            if SequenceMatcher(None, org_word, predicted_token).ratio() \\\n",
        "                < 0.6:\n",
        "                if org_word not in list(set(det + prep\n",
        "                        + helping_verbs)) or predicted_token \\\n",
        "                    not in list(set(det + prep + helping_verbs)):\n",
        "                    continue\n",
        "            if org_word == predicted_token:\n",
        "                continue\n",
        "        else:\n",
        "            # case for MASKed spaces\n",
        "            mask = False\n",
        "\n",
        "            # only allow determiners / prepositions  / helping verbs in spaces\n",
        "            if predicted_token in list(set(det + prep + helping_verbs)):\n",
        "                text[mask_index] = predicted_token\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        text.remove('[SEP]')\n",
        "        text.remove('[CLS]')\n",
        "        new_sent = ' '.join(text)\n",
        "\n",
        "        #print(new_sent)\n",
        "        # retain new sentences which have a\n",
        "        # minimum chance of correctness using BERT GED\n",
        "\n",
        "        (no_error, prob_val) = check_GE([new_sent])\n",
        "        exps = [np.exp(i) for i in prob_val[0]]\n",
        "        sum_of_exps = sum(exps)\n",
        "        softmax = [j / sum_of_exps for j in exps]\n",
        "        if no_error and softmax[1] > 0.996:\n",
        "            print ('*', '')\n",
        "            new_sentences.append(new_sent)\n",
        "\n",
        "    print ('')\n",
        "\n",
        "    # remove duplicate suggestions\n",
        "\n",
        "    spelling_sentences = []\n",
        "    [spelling_sentences.append(sent) for sent in new_sentences]\n",
        "    spelling_sentences = list(dict.fromkeys(spelling_sentences))\n",
        "    spelling_sentences\n",
        "\n",
        "    return spelling_sentences\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqzQ6SBF49fv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a542e46-e5e1-41bc-d548-438d9032a367"
      },
      "source": [
        "for sent in org_text:\n",
        "\n",
        "    print ('Input Sentence >>> ' + sent)\n",
        "\n",
        "    sentences = create_spelling_set(sent)\n",
        "    spelling_sentences = create_grammar_set(sentences)\n",
        "    sentences = create_mask_set(spelling_sentences)\n",
        "\n",
        "    print ('processing {0} possibilities'.format(len(sentences)))\n",
        "\n",
        "    sentences = check_grammar(sent, sentences, spelling_sentences)\n",
        "\n",
        "    print ('Suggestions & Probabilities')\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "        print ('None')\n",
        "        continue\n",
        "\n",
        "    (no_error, prob_val) = check_GE(sentences)\n",
        "\n",
        "    for i in range(len(prob_val)):\n",
        "        exps = [np.exp(i) for i in prob_val[i]]\n",
        "        sum_of_exps = sum(exps)\n",
        "        softmax = [j / sum_of_exps for j in exps]\n",
        "        print ('{0} - {1:0.4f}%'.format(sentences[i], softmax[1] * 100))\n",
        "\n",
        "    print ('-')\n",
        "    print ()\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence >>> They drank the pub .\n",
            "processing 10 possibilities\n",
            "* \n",
            "\n",
            "Suggestions & Probabilities\n",
            "they drank at the pub . - 99.8071%\n",
            "-\n",
            "\n",
            "Input Sentence >>> I am looking forway to see you soon .\n",
            "processing 126 possibilities\n",
            "* \n",
            "* \n",
            "* \n",
            "* \n",
            ".\n",
            "* \n",
            "! exception\n",
            "* \n",
            ".\n",
            "* \n",
            "! exception\n",
            "* \n",
            "* \n",
            "* \n",
            "\n",
            "Suggestions & Probabilities\n",
            "i am looking forward to see you soon . - 99.7493%\n",
            "i am looking to Norway to see you soon . - 99.7757%\n",
            "i am looking for a way to see you soon . - 99.7564%\n",
            "i am looking forward to seeing you soon . - 99.7722%\n",
            "am i looking forward to see you soon . - 99.6467%\n",
            "i look forward to see you soon . - 99.7621%\n",
            "-\n",
            "\n",
            "Input Sentence >>> The cat sat at mat .\n",
            "processing 12 possibilities\n",
            "* \n",
            "\n",
            "Suggestions & Probabilities\n",
            "the cat sat at the mat . - 99.8284%\n",
            "-\n",
            "\n",
            "Input Sentence >>> Giant otters is an apex predator .\n",
            "processing 26 possibilities\n",
            "* \n",
            "* \n",
            "\n",
            "Suggestions & Probabilities\n",
            "giant otters are an apex predator . - 99.8082%\n",
            "-\n",
            "\n",
            "Input Sentence >>> There is no a doubt, tracking system has brought many benefits in this information age .\n",
            "processing 152 possibilities\n",
            ".\n",
            "* \n",
            "* \n",
            ".\n",
            ".\n",
            "\n",
            "Suggestions & Probabilities\n",
            "there is no doubt, the tracking system has brought many benefits in this information age . - 99.6639%\n",
            "there is no doubt, tracking the system has brought many benefits in this information age . - 99.6644%\n",
            "-\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckDFxELX5vT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1bf0ff48-8afa-47f7-f399-2281ce8a594e"
      },
      "source": [
        "org_text = []\n",
        "org_text.append(\"They drank the pub .\")\n",
        "\n",
        "for sent in org_text:\n",
        "\n",
        "    print ('Input Sentence >>> ' + sent)\n",
        "\n",
        "    sentences = create_spelling_set(sent)\n",
        "    spelling_sentences = create_grammar_set(sentences)\n",
        "    sentences = create_mask_set(spelling_sentences)\n",
        "\n",
        "    print ('processing {0} possibilities'.format(len(sentences)))\n",
        "\n",
        "    sentences = check_grammar(sent, sentences, spelling_sentences)\n",
        "\n",
        "    print ('Suggestions & Probabilities')\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "        print ('None')\n",
        "        continue\n",
        "\n",
        "    (no_error, prob_val) = check_GE(sentences)\n",
        "\n",
        "    for i in range(len(prob_val)):\n",
        "        exps = [np.exp(i) for i in prob_val[i]]\n",
        "        sum_of_exps = sum(exps)\n",
        "        softmax = [j / sum_of_exps for j in exps]\n",
        "        print ('{0} - {1:0.4f}%'.format(sentences[i], softmax[1] * 100))\n",
        "\n",
        "    print ('-')\n",
        "    print ()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence >>> They drank the pub .\n",
            "processing 10 possibilities\n",
            "* \n",
            "\n",
            "Suggestions & Probabilities\n",
            "they drank at the pub . - 99.8071%\n",
            "-\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpWbA5uq8qWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}