# Python | Data Science | Machine Learning

### Type of the problem: 
    - Whether regression, classification, clustering etc.
    - supervised, unsupervised, reinforcement etc.
    - 
    
1. Import required libraries
2. Load data -> pandas + DataFrame
    - csv, excel, hdfs, stata etc.
    - chunksize, nrows, low_memory, na_values
    - study the data
    - whether adequate data is available
    - features and target(s)
    - 
    
3. Imputation
    - strategy for treating missing values
        -- ffill, bfill, mean, map(key:value)
    - strategy for handling outliers
    	-- cause of outliers
        -- uni/bi variate outliers
        -- 2-3 times less or more than S.Dev
        -- 1.5-2 times IQR
        -- exclude points below 5th percentiles & above 95th 
        -- 
        
4. Exploratory Data Analysis
    - analyzing data sets to summarize their main characteristics, often with visual methods
    - central tendency
    - measure of dispersion
    - measure of spread (variance)
    - heatmap of correlation
    - univariate analysis
    - bivariate analysis
    - multivariate analysis
    - data cleaning and wrangling (tidying)
    - correlation -- df.corr() or seaborn heatmap
    - distribution -- df.hist()
    - density dist -- df.plot(kind='density', subplots=True, 
                            layout=(3,3), sharex=False)
    - scatter matrix of df
    - summarization
    - visualization
    
    
5. Statistical Modeling + Hypothesis Testing
    - perform required statistical tests
    - multi-colinearity
    	- Condition number in statsmodels.api OLS summary
    - OLS summary
    	- sm.OLS(y, X).fit(), summary
	# 1st table
		- Dependent variable
		- Number of observations
		- Degrees of freedom
		- R-squared
			- percentage of variance, ie., if R²=93.5% then the
		  		model explains 93.5% of the variance in our dependent variable
			- adding variables to a regression model, in turn increases R²
			- 
		
	# 2nd table
		- coef of Independent variables ()
    			- as IV's inc by 1, DV reduces by the corresponding value
		- Standard error
			- standard deviation of test statistic (eg. mean)
		- t-score
			- significance for hypothesis test
		- p-values
			- significance for hypothesis test
		- confidence intervals
			- with 95% confidence we predict 
				that IV's lies between the two values for 0.025 and 0.975
	
	# 3rd table
		- Condition number
			- larger value indicates multicolinearity
			- values over 20 are to be concerned of
		- 
		
6. Feature reduction
    - Standardization
        - StandardScaler
    - PCA, ICA
    - SelectPercentile
    - Combine features 
    - Collinear features
    - Ignore unwanted features
    - Data with low variance can't help in prediction
        [1,2,1,1,3,1,1,2,1,2,1,1,2,1,1]
    - Dimensionality reduction with t-SNE
    	- t-Distributed Stochastic Neighbor Embedding (t-SNE)
    -
    
7. Label encoding or One Hot encoding
    - note label encoded data
    - check the MSE and other errors before and after encoding
    	- for different random_state's
    
8. Train test split
    - random state
    - test size
    - cross validation cv
        - shuffle split
        - GridSearchCV
        - cross_val_score
        - 
        
9. Algorithms
    - choice of algorithms
    - advantages
    - disadvantages
    - calibration
    - 
    
10. Modeling
    - Train and test
    - Predict
    - Measure Accuracy
    	- mean_squared_error
	- r2_score
	- confusion_matrix
	- 
    - Cost function
    	- Gradient Descent
    - Parameter Tuning
    - Retrain
    - 

Also, note 

*  ROC curve
*  Precision = TP / (TP + FP)
*  Recall    = TP / (TP + FN)
*  Confusion matrix 
	    PY   PN
	AY  [TP, FP]
	AN  [FN, TN]
 
*  True +ve, False +ve

*  Normalization
 
*  Model fit and transform
 
*  Variable transformation
    - scale data
    - symmetric distribution
    - complex non-linear relationships into linear relationships
    - 
