# Python | Data Science | Machine Learning

### Type of the problem: 
    - Whether regression, classification, clustering etc.
    - supervised, unsupervised, reinforcement etc.
    - by examining the 'TARGET' column
    - 
    
1. Import required libraries
2. Load data -> pandas + DataFrame
    - csv, excel, hdfs, stata etc.
    - chunksize, nrows, low_memory, na_values
    - study the data
    - prepare data dictionary
    - whether adequate data is available
    - features and target(s)
    - 
    
3. Imputation
    - check the importance of outliers/anomailes
    - strategy for treating missing values
    	-- use Imputer from sklearn.preprocessing
        -- ffill, bfill, mean, map(key:value)
	-- df = df.groupby(df.columns, axis = 1)
		.transform(lambda x: x.fillna(x.mean()))
    - strategy for handling outliers
    	-- cause of outliers
        -- uni/bi variate outliers
        -- 2-3 times less or more than S.Dev
        -- 1.5-2 times IQR
        -- exclude points below 5th percentiles & above 95th 
        -- 
        
4. Exploratory Data Analysis
    - to learn what data can tell us
    - analyzing data sets to summarize their main characteristics, often with visual methods
    - central tendency
    - standard deviation
    - measure of dispersion
    - measure of spread (variance)
    - heatmap of correlation
    - univariate analysis
    - bivariate analysis
    - multivariate analysis
    - find the skewness
    	- might be required to transform
    - data cleaning and wrangling (tidying)
    - correlation -- df.corr() or seaborn heatmap
    	- Pearson's
	- Spearman's
	- Kendall's rank (tau)
    - distribution -- df.hist()
    - density dist -- df.plot(kind='density', subplots=True, 
                            layout=(3,3), sharex=False)
    - scatter matrix of df
    - summarization
    - visualization
	- bar, hist, box, scatter, pair, facetgrid, 
	- kdeplot, heatmap, line, area, dispersion,
	- lmplot, regplot
    
    
5. Statistical Modeling + Hypothesis Testing
    - perform required statistical tests
    - chi-squared
    	- test whether two categorical variables are related or independent
    - t-test
    	- whether the means of 2 independent samples are significantly different
    - ANOVA
    	- whether the means of 3 or more independent samples are significantly different
    - multi-colinearity
    	- Condition number in statsmodels.api OLS summary
    - OLS summary
    	- sm.OLS(y, X).fit(), summary
	# 1st table
		- Dependent variable
		- Number of observations
		- Degrees of freedom
		- R-squared
			- percentage of variance, ie., if R²=93.5% then the
		  		model explains 93.5% of the variance in our dependent variable
			- adding variables to a regression model, in turn increases R²
			- 
		
	# 2nd table
		- coef of Independent variables ()
    			- as IV's inc by 1, DV reduces by the corresponding value
		- Standard error
			- standard deviation of test statistic (eg. mean)
		- t-score
			- significance for hypothesis test
		- p-values
			- significance for hypothesis test
		- confidence intervals
			- with 95% confidence we predict 
				that IV's lies between the two values for 0.025 and 0.975
	
	# 3rd table
		- Condition number
			- larger value indicates multicolinearity
			- values over 20 are to be concerned of
		- 
		
6. Feature engineering
    - make sure data is as relevant to the task as possible
    - different types
    	- identifying relevant features
	- creating new features
	- polynomial features
	    - degree of the polynomial
	    	- as degree increases, so does overfitting
	    - interaction terms
	      eg. features x1, x2 -> x1*x2^2
	- domain knowledge features
	- 
	
    - Standardization
        - StandardScaler
    - Normalization
    	- MinMaxScaler
    - PCA, ICA
    - Linear Discriminant Analysis
    	- supervised dimensionality reduction technique
	- similar to PCA, +advantage of tackling overfitting
    - SelectPercentile
    - Combine features 
    - Collinear features
    - Ignore unwanted features
    - Data with low variance can't help in prediction
        [1,2,1,1,3,1,1,2,1,2,1,1,2,1,1]
    - Dimensionality reduction with t-SNE
    	- t-Distributed Stochastic Neighbor Embedding (t-SNE)
    -
    
7. Encoding
    - note label encoded data
    - check the MSE and other errors before and after encoding
    	- for different random_state's
    - before encoding, ensure that missing values are handled
    
    - Label encoding
    	- arbitrary ordering
    - One Hot encoding
    	- different columns with 0 and 1
	- get_dummies(<pandas dataframe>)
	-
	
    - DictVectorizer
    - Binary encoder
    - judicious combination of OHE with PCA for dim. red. 
    	can seldom be beat by other encoding schemes 
    - inverse_transform 
    	- going back from numerical to string
	
8. Train test split
    - random state
    - test size
    - cross validation cv
        - shuffle split
        - GridSearchCV
        - cross_val_score
        - 
        
9. Algorithms
    - choice of algorithms
    - advantages
    - disadvantages
    - calibration
    - plot feature importances
    	- RandomForestClassifier().feature_importances_
    - 

10. Modeling
    - Train and test
    - Predict
    - Measure Accuracy
    	- mean_squared_error
	- r2_score
	- confusion_matrix
	- 
    - Cost function
    	- Gradient Descent
    - Parameter Tuning
    - Retrain
    - 

11. Reducing Variance in final model
	- sources of variance
		- noise in training data
		- randomness of ML algorithm
	- measure Variance introduced by algorithm
		- repeated runs
	- measure Variance introduced by training data
		- train on different samples
	- ensemble models
	- sensitivity analysis
	- more training data
	- 

Also, note 

*  ROC curve - FP vs TP
	- guessing model will have ROC of 0.5
	- 
	
*  Precision = TP / (TP + FP)
*  Recall    = TP / (TP + FN)
*  Confusion matrix 
	    PY   PN
	AY  [TP, FP]
	AN  [FN, TN]
 
*  True +ve, False +ve
 
*  Model fit and transform
 
*  Variable transformation
    - scale data
    - symmetric distribution
    - complex non-linear relationships into linear relationships
    - 
