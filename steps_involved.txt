# Python | Data Science | Machine Learning

### Type of the problem: 
    - Whether regression, classification, clustering etc.
    - supervised, unsupervised, reinforcement etc.
    - by examining the 'TARGET' column
    - Model types
    	- logical, geometric or probabilistic

Summary of steps
- Handling data
- Exploratory Data Analysis
- Statistical Modeling + Hypothesis Testing
- Feature engineering
- Algorithms
- Modeling
- Other steps
    - Reducing Variance in final model

Note: report findings and observations at each stage, textually or visually

1. Handling data
    - Import required libraries
    - Load data -> pandas + DataFrame
    	- csv, excel, hdfs, stata etc.
    	- chunksize, nrows, low_memory, na_values
    	- study the data
    	- prepare data dictionary
    	- whether adequate data is available
    	- features and target(s)
    	- 
    
    Missing values & outliers
    	Imputation
    	- check the importance of outliers/anomalies
    	- strategy for treating missing values
    		-- use Imputer from sklearn.preprocessing
        	-- ffill, bfill, mean, map(key:value)
		-- df = df.groupby(df.columns, axis = 1)
			.transform(lambda x: x.fillna(x.mean()))
    	- strategy for handling outliers
		-- cause of outliers
		-- uni/bi variate outliers
		-- 2-3 times less or more than S.Dev
		-- 1.5-2 times IQR
		-- exclude points below 5th percentiles & above 95th 
		-- 
        
2. Exploratory Data Analysis
    - to learn what data can tell us
    - analyzing data sets to summarize their main characteristics, often with visual methods
    - central tendency
    - standard deviation
    - measure of dispersion
    - measure of spread (variance)
    - heatmap of correlation
    - univariate analysis
    - bivariate analysis
    - multivariate analysis
    - find the skewness
    	- might be required to transform
    - data cleaning and wrangling (tidying)
    - correlation -- df.corr() or seaborn heatmap
    	- Pearson's
	- Spearman's
	- Kendall's rank (tau)
    - distribution -- df.hist()
    - density dist -- df.plot(kind='density', subplots=True, 
                            layout=(3,3), sharex=False)
    - scatter matrix of df
    - summarization
    - visualization
	- bar, hist, box, scatter, pair, facetgrid, 
	- kdeplot, heatmap, line, area, dispersion,
	- lmplot, regplot
    
    Encoding
    	- note label encoded data
    	- check the MSE and other errors before and after encoding
		- for different random_state's
    	- before encoding, ensure that missing values are handled
	
	- get_dummies(<pandas dataframe>)
	- factorize columns (<pd.factorize(df['Gender'])>)

    	- Label encoding
		- arbitrary ordering
    	- One Hot encoding
		- non-arbitrary encoding
		- 

    	- DictVectorizer
    	- Binary encoder
    	- judicious combination of OHE with PCA for dim. red. 
		can seldom be beat by other encoding schemes 
    	- inverse_transform 
		- going back from numerical to string

3. Statistical Modeling + Hypothesis Testing
    - perform required statistical tests
    - chi-squared
    	- test whether two categorical variables are related or independent
    - t-test
    	- whether the means of 2 independent samples are significantly different
    - ANOVA
    	- whether the means of 3 or more independent samples are significantly different
    - multi-colinearity
    	- Condition number in statsmodels.api OLS summary
    - OLS summary
    	- sm.OLS(y, X).fit(), summary
	# 1st table
		- Dependent variable
		- Number of observations
		- Degrees of freedom
		- R-squared
			- percentage of variance, ie., if R²=93.5% then the
		  		model explains 93.5% of the variance in our dependent variable
			- adding variables to a regression model, in turn increases R²
			- 
		
	# 2nd table
		- coef of Independent variables ()
    			- as IV's inc by 1, DV reduces by the corresponding value
		- Standard error
			- standard deviation of test statistic (eg. mean)
		- t-score
			- significance for hypothesis test
		- p-values
			- significance for hypothesis test
		- confidence intervals
			- with 95% confidence we predict 
				that IV's lies between the two values for 0.025 and 0.975
	
	# 3rd table
		- Condition number
			- larger value indicates multicolinearity
			- values over 20 are to be concerned of
		- 
		
4. Feature engineering
    - make sure data is as relevant to the task as possible
    - different types
    	- identifying relevant features
	- creating new features
	- polynomial features
	    - degree of the polynomial
	    	- as degree increases, so does overfitting
	    - interaction terms
	      eg. features x1, x2 -> x1*x2^2
	- domain knowledge features
	- 
	
    - Standardization
        - StandardScaler
    - Normalization
    	- MinMaxScaler
    - PCA, ICA
    - Linear Discriminant Analysis
    	- supervised dimensionality reduction technique
	- similar to PCA, +advantage of tackling overfitting
    - SelectPercentile
    - Combine features 
    - Collinear features
    - Ignore unwanted features
    - Data with low variance can't help in prediction
        [1,2,1,1,3,1,1,2,1,2,1,1,2,1,1]
    - Dimensionality reduction with t-SNE
    	- t-Distributed Stochastic Neighbor Embedding (t-SNE)
    - 
    	
5. Algorithms
    - choice of algorithms
    - advantages
    - disadvantages
    
    Train test split
    	- random state
    	- test size
    	- cross validation cv
        	- shuffle split
        	- GridSearchCV
        	- cross_val_score
        	- 
    - calibration
    - plot feature importances
    	- RandomForestClassifier().feature_importances_
    - 

6. Modeling
    - Train and test
    - Predict
    - Measure Accuracy
    	- mean_squared_error
	- r2_score
	- confusion_matrix
	- 
    - baseline model
    - improved model
    - model interpretations
    
    - Cost function
    	- Gradient Descent
    - Parameter Tuning
    - Retrain
    - Overfitting/underfitting
	- underfitting - a decision boundary that hasn't latched onto the true border enough, 
		 ie., when it doesn't take into consideration enough information to accurately model the actual data
	- overfitting - as opposed to being too tightly wrapped against individual points
    		- it's observed numerically when the testing error does not reflect the training error
		- model has too many parameters, it is susceptible to overfitting
		- avoid overfitting by adding more iterations/more parameters
    - 

7. Other steps

   a) Reducing Variance in final model
	- sources of variance
		- noise in training data
		- randomness of ML algorithm
	- measure Variance introduced by algorithm
		- repeated runs
	- measure Variance introduced by training data
		- train on different samples
	- ensemble models
	- sensitivity analysis
	- more training data
	- 

    Also, note 
    
    *  Optimizing errors/loss functions
	- cross entropy
	- MSE
	- 

    *  ROC curve - FP vs TP
	- guessing model will have ROC of 0.5
	- 
	
    *  Precision = TP / (TP + FP)
    *  Recall    = TP / (TP + FN)
    *  Confusion matrix 
	    PY   PN
	AY  [TP, FP]
	AN  [FN, TN]
 
    *  True +ve, False +ve
 
    *  Model fit and transform
 
    *  Variable transformation
    	- scale data
    	- symmetric distribution
    	- complex non-linear relationships into linear relationships
    	- 
